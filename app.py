
import streamlit as st
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from threading import Thread

st.set_page_config(page_title="ä¸­æ–‡å®¢æœç”Ÿæˆ (RAG + æŒ‡ä»¤å¾®èª¿æ¨¡å‹)", page_icon="ğŸ’¬", layout="wide")

# -----------------------
# Sidebar: æ¨¡å‹ & åƒæ•¸
# -----------------------
with st.sidebar:
    st.markdown("## æ¨¡å‹è¨­å®š")
    model_id = st.text_input(
        "Hugging Face æ¨¡å‹ï¼ˆå»ºè­°ï¼šQwen/Qwen2.5-0.5B-Instruct æˆ– 1.5Bï¼‰",
        value="Qwen/Qwen2.5-0.5B-Instruct"
    )
    use_4bit = st.checkbox("ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆéœ€å®‰è£ bitsandbytesï¼›é›²ç«¯ä¸ä¸€å®šæ”¯æ´ï¼‰", value=False)
    max_new_tokens = st.slider("å›è¦†é•·åº¦ (max_new_tokens)", 64, 1024, 256, 16)
    temperature = st.slider("æº«åº¦ (temperature)", 0.0, 1.5, 0.3, 0.1)
    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.05)
    st.divider()
    st.markdown("### æª¢ç´¢è¨­å®š")
    top_k = st.slider("å–å›çŸ¥è­˜æ¢ç›®æ•¸ (top_k)", 1, 10, 3, 1)
    use_query_rewrite = st.checkbox("å…ˆé‡å¯«ç”¨æˆ¶å•é¡Œï¼ˆå¯æå‡æª¢ç´¢æº–ç¢ºåº¦ï¼‰", value=True)

st.title("ğŸ’¬ ä¸­æ–‡å®¢æœç”Ÿæˆ (RAG + æŒ‡ä»¤æ¨¡å‹)")
st.caption("ä¸Šå‚³ FAQ/çŸ¥è­˜åº« â†’ æª¢ç´¢ â†’ æ³¨å…¥æ¨¡å‹æç¤º â†’ ç”¢ç”Ÿå®¢æœå›è¦†ã€‚é è¨­æ”¯æ´ä¸­æ–‡ã€‚")

# -----------------------
# å…§å»ºå°å‹FAQç¤ºç¯„
# -----------------------
DEFAULT_FAQ = pd.DataFrame(
    [
        {"question":"ä½ å€‘çš„ç‡Ÿæ¥­æ™‚é–“æ˜¯ï¼Ÿ","answer":"æˆ‘å€‘çš„å®¢æœæ™‚é–“ç‚ºé€±ä¸€è‡³é€±äº” 09:00â€“18:00ï¼ˆåœ‹å®šå‡æ—¥é™¤å¤–ï¼‰ã€‚"},
        {"question":"å¦‚ä½•ç”³è«‹é€€è²¨ï¼Ÿ","answer":"è«‹æ–¼åˆ°è²¨ 7 å¤©å…§é€éè¨‚å–®é é¢é»é¸ã€ç”³è«‹é€€è²¨ã€ï¼Œç³»çµ±å°‡å¼•å°æ‚¨å®Œæˆæµç¨‹ã€‚"},
        {"question":"é‹è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ","answer":"å–®ç­†è¨‚å–®æ»¿ NT$ 1000 å…é‹ï¼Œæœªæ»¿å‰‡é…Œæ”¶ NT$ 80ã€‚"},
        {"question":"å¯ä»¥é–‹ç«‹ç™¼ç¥¨å—ï¼Ÿ","answer":"æˆ‘å€‘æä¾›é›»å­ç™¼ç¥¨ï¼Œè«‹æ–¼çµå¸³æ™‚å¡«å¯«çµ±ä¸€ç·¨è™Ÿèˆ‡æŠ¬é ­ã€‚"},
    ]
)

# -----------------------
# Session state
# -----------------------
if "faq_df" not in st.session_state:
    st.session_state.faq_df = DEFAULT_FAQ.copy()
if "index" not in st.session_state:
    st.session_state.index = None
if "embedder" not in st.session_state:
    st.session_state.embedder = None
if "model" not in st.session_state:
    st.session_state.model = None
    st.session_state.tokenizer = None
if "history" not in st.session_state:
    st.session_state.history = []  # list of dicts: {"role":"user/assistant", "content":str}

# -----------------------
# ä¸Šå‚³ FAQ/çŸ¥è­˜åº«
# -----------------------
st.subheader("æ­¥é©Ÿ1ï¼šä¸Šå‚³ FAQ / çŸ¥è­˜åº« (CSV)")
st.write("éœ€è¦å…©æ¬„ï¼š`question, answer`ï¼ˆUTF-8ï¼‰ã€‚è‹¥æœªä¸Šå‚³ï¼Œæœƒä½¿ç”¨ç¤ºç¯„è³‡æ–™ã€‚")
uploaded = st.file_uploader("ä¸Šå‚³ CSV", type=["csv"])
if uploaded is not None:
    try:
        df = pd.read_csv(uploaded)
        assert set(["question", "answer"]).issubset(df.columns), "CSV éœ€åŒ…å« question èˆ‡ answer æ¬„ä½"
        st.session_state.faq_df = df.dropna().reset_index(drop=True)
        st.success(f"å·²è¼‰å…¥ {len(df)} ç­† FAQã€‚")
    except Exception as e:
        st.error(f"è®€å–CSVå¤±æ•—ï¼š{e}")

with st.expander("æŸ¥çœ‹ç›®å‰ FAQ è³‡æ–™", expanded=False):
    st.dataframe(st.session_state.faq_df, use_container_width=True)

# -----------------------
# å»ºç«‹å‘é‡ç´¢å¼•
# -----------------------
st.subheader("æ­¥é©Ÿ2ï¼šå»ºç«‹/æ›´æ–° æª¢ç´¢ç´¢å¼•")
if st.button("å»ºç«‹ç´¢å¼• / é‡æ–°ç´¢å¼•"):
    with st.spinner("æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•..."):
        # å¤šèªæ¨¡å‹ï¼Œæ”¯æ´ä¸­æ–‡
        st.session_state.embedder = SentenceTransformer("intfloat/multilingual-e5-small")
        # e5 small ç”¨æ³•ï¼šéœ€è¦åœ¨å‰é¢åŠ ä¸Š "query: " / "passage: "
        corpus = ["passage: " + str(q) + " " + str(a) for q, a in zip(st.session_state.faq_df["question"], st.session_state.faq_df["answer"])]
        emb = st.session_state.embedder.encode(corpus, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)
        dim = emb.shape[1]
        index = faiss.IndexFlatIP(dim)  # å…§ç©=cosine (å› å·² normalize)
        index.add(emb)
        st.session_state.index = index
    st.success("ç´¢å¼•å®Œæˆï¼")

# -----------------------
# è¼‰å…¥æ–‡å­—ç”Ÿæˆæ¨¡å‹
# -----------------------
st.subheader("æ­¥é©Ÿ3ï¼šè¼‰å…¥æ–‡å­—ç”Ÿæˆæ¨¡å‹")
def load_model_tokenizer(model_id: str, use_4bit: bool=False):
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_id, device_map="auto", quantization_config=bnb_config, trust_remote_code=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id, device_map="auto", torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True
        )
    return tokenizer, model

if st.button("è¼‰å…¥/æ›´æ–° æ¨¡å‹"):
    with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹èˆ‡æ¬Šé‡...ï¼ˆé¦–æ¬¡è¼‰å…¥éœ€è¼ƒä¹…ï¼‰"):
        try:
            tok, mdl = load_model_tokenizer(model_id, use_4bit)
            st.session_state.tokenizer = tok
            st.session_state.model = mdl
            st.success(f"å·²è¼‰å…¥ï¼š{model_id}")
        except Exception as e:
            st.error(f"è¼‰å…¥å¤±æ•—ï¼š{e}")

# -----------------------
# æª¢ç´¢è¼”åŠ©ï¼šé‡å¯«æŸ¥è©¢ï¼ˆé¸ç”¨ï¼‰
# -----------------------
def rewrite_query(q: str) -> str:
    # ç°¡å–®å•Ÿç™¼å¼é‡å¯«ï¼šç§»é™¤è´…è©ã€ç¹ç°¡æ··åˆæ¸…ç†ï¼Œå¯æ›æˆå°æ¨¡å‹ç”Ÿæˆï¼ˆæ­¤è™•ä¿æŒé›¢ç·šï¼‰
    q = q.strip()
    for t in ["è«‹å•", "éº»ç…©", "ä¸€ä¸‹", "å¯ä»¥", "èƒ½å¦", "æƒ³å•", "è¬è¬", "è¬äº†", "å“ˆå›‰", "æ‚¨å¥½", "ä½ å¥½"]:
        q = q.replace(t, "")
    return q

# -----------------------
# ç”¢ç”Ÿå›è¦†ï¼ˆæµå¼ï¼‰
# -----------------------
def generate_stream(prompt, max_new_tokens=256, temperature=0.3, top_p=0.9):
    tok = st.session_state.tokenizer
    mdl = st.session_state.model
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)
    streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=True)
    gen_kwargs = dict(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True if temperature > 0 else False,
        temperature=temperature,
        top_p=top_p,
        streamer=streamer
    )
    thread = Thread(target=mdl.generate, kwargs=gen_kwargs)
    thread.start()
    partial = ""
    for new_text in streamer:
        partial += new_text
        yield partial

# -----------------------
# Chat å€å¡Š
# -----------------------
st.subheader("æ­¥é©Ÿ4ï¼šé–‹å§‹å°è©±")
user_input = st.text_input("è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆä¸­æ–‡ï¼‰", placeholder="ä¾‹å¦‚ï¼šæƒ³é€€è²¨è¦æ€éº¼åšï¼Ÿ")

col1, col2 = st.columns([3,1])
with col1:
    send = st.button("é€å‡º", use_container_width=True)
with col2:
    clear = st.button("æ¸…é™¤å°è©±", use_container_width=True)

if clear:
    st.session_state.history = []
    st.experimental_rerun()

# é¡¯ç¤ºå°è©±æ­·å²
for msg in st.session_state.history:
    with st.chat_message("user" if msg["role"]=="user" else "assistant"):
        st.markdown(msg["content"])

def build_prompt(query, retrieved):
    sys = (
        "ä½ æ˜¯å°ˆæ¥­ã€è€å¿ƒçš„ä¸­æ–‡å®¢æœã€‚"
        "è«‹æ ¹æ“šã€çŸ¥è­˜åº«ã€æä¾›å¯é ä¸”ç°¡æ½”çš„å›ç­”ï¼Œå¿…è¦æ™‚æå‡ºå¯è¡Œæ­¥é©Ÿã€‚"
        "è‹¥çŸ¥è­˜åº«ç„¡ç­”æ¡ˆï¼Œè«‹å¦èª èªªæ˜ä¸¦æä¾›äººå·¥å”åŠ©ç®¡é“ï¼ˆä¾‹å¦‚å®¢æœä¿¡ç®±/å·¥å–®æµç¨‹çš„ä½”ä½æè¿°ï¼‰ã€‚"
        "ç¦æ­¢æé€ è¨‚å–®æˆ–å€‹è³‡ï¼›ä¸è¦è¦æ±‚æä¾›ä¿¡ç”¨å¡ç­‰æ•æ„Ÿè³‡è¨Šã€‚"
    )
    kb_text = "\\n\\n".join([f"[{i+1}] Q: {r['q']}\\nA: {r['a']}" for i, r in enumerate(retrieved)])
    chat_history = "\\n".join([f"{'å®¢æˆ¶' if m['role']=='user' else 'å®¢æœ'}: {m['content']}" for m in st.session_state.history[-6:]])
    prompt = f"""<|system|>
{sys}
<|knowledge_base|>
{kb_text if kb_text else 'ï¼ˆç›®å‰æ²’æœ‰å¯ç”¨çš„çŸ¥è­˜åº«æ¢ç›®ï¼‰'}
<|history|>
{chat_history}
<|user|>
{query}
<|assistant|>"""
    return prompt

if send and user_input:
    # 1) æŸ¥è©¢é‡å¯«
    q = rewrite_query(user_input) if use_query_rewrite else user_input

    # 2) æª¢ç´¢
    retrieved = []
    if st.session_state.index is not None and st.session_state.embedder is not None:
        q_emb = st.session_state.embedder.encode(["query: " + q], convert_to_numpy=True, normalize_embeddings=True)
        D, I = st.session_state.index.search(q_emb, top_k)
        for idx in I[0]:
            if 0 <= idx < len(st.session_state.faq_df):
                row = st.session_state.faq_df.iloc[idx]
                retrieved.append({"q": str(row["question"]), "a": str(row["answer"])})
    else:
        # è‹¥å°šæœªç´¢å¼•ï¼Œä½¿ç”¨å‰ä¸‰ç­†ç¤ºä¾‹
        df = st.session_state.faq_df.head(top_k)
        for _, row in df.iterrows():
            retrieved.append({"q": str(row["question"]), "a": str(row["answer"])})

    # 3) å»ºç«‹æç¤ºè©
    prompt = build_prompt(user_input, retrieved)

    st.session_state.history.append({"role":"user", "content": user_input})
    with st.chat_message("assistant"):
        if st.session_state.model is None or st.session_state.tokenizer is None:
            st.warning("å°šæœªè¼‰å…¥æ¨¡å‹ã€‚è«‹åœ¨å´é‚Šæ¬„é»æ“Šã€è¼‰å…¥/æ›´æ–° æ¨¡å‹ã€ã€‚")
        else:
            # æµå¼è¼¸å‡º
            holder = st.empty()
            final_text = ""
            for partial in generate_stream(prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p):
                final_text = partial
                holder.markdown(final_text)
            st.session_state.history.append({"role":"assistant", "content": final_text})

st.markdown("---")
st.caption("å°è²¼å£«ï¼š")
st.caption("1) å…ˆç”¨ FAQ åš RAGï¼Œå¯å¿«é€Ÿä¸Šç·šï¼›2) ä¹‹å¾Œå†ç”¨ JDDC/CSDS ç­‰ä¸­æ–‡å®¢æœè³‡æ–™åš SFT å¾®èª¿ï¼›3) é›²ç«¯éƒ¨ç½²å¯ç”¨ Streamlit Cloud / Hugging Face Spacesã€‚")
